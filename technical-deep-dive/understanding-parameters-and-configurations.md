# Understanding parameters and configurations

When using ChatGPT, there are several parameters you can adjust to control its output:

1. **Max tokens:** This is the maximum length of the model's response. If you set this to a low number, the model might not complete its thought before being cut off.
2. **Temperature:** This controls the randomness of the model's output. A higher value like 0.8 makes the output more random, while a lower value like 0.2 makes it more deterministic and focused.
3. **Top P:** Also known as nucleus sampling, this parameter helps control the diversity of the model's output. A smaller value will make the output more focused and predictable, while a more significant value will increase randomness.
